<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Practically Insignificant  | Bounded Bayes: Markov Chain Monte Carlo (MCMC) for Posteriors of Bounded Parameters</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="generator" content="Hugo 0.120.4">
    
    
      <META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">
    

    
    
      <link href="/dist/css/app.d98f2eb6bcd1eaedb7edf166bd16af26.css" rel="stylesheet">
    

    

    
      
    

    
    
    <meta property="og:title" content="Bounded Bayes: Markov Chain Monte Carlo (MCMC) for Posteriors of Bounded Parameters" />
<meta property="og:description" content="This is largely a note to my past-self on how to easily use Markov Chain Monte Carlo (MCMC) methods for Bayesian inference when the parameter you are interested in has bounded support. The most basic MCMC methods involve using additive noise to get new draws, which can cause problems if that kicks you out of the parameter space.
Suggestions abound to use the transformation trick on a bounded parameter \(\theta\), and then make draws of the transformed parameter." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/post/bounded-bayes/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2020-12-17T00:00:00+00:00" />
<meta property="article:modified_time" content="2020-12-17T00:00:00+00:00" />

<meta itemprop="name" content="Bounded Bayes: Markov Chain Monte Carlo (MCMC) for Posteriors of Bounded Parameters">
<meta itemprop="description" content="This is largely a note to my past-self on how to easily use Markov Chain Monte Carlo (MCMC) methods for Bayesian inference when the parameter you are interested in has bounded support. The most basic MCMC methods involve using additive noise to get new draws, which can cause problems if that kicks you out of the parameter space.
Suggestions abound to use the transformation trick on a bounded parameter \(\theta\), and then make draws of the transformed parameter."><meta itemprop="datePublished" content="2020-12-17T00:00:00+00:00" />
<meta itemprop="dateModified" content="2020-12-17T00:00:00+00:00" />
<meta itemprop="wordCount" content="769">
<meta itemprop="keywords" content="statistics,reminders,worked examples," /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Bounded Bayes: Markov Chain Monte Carlo (MCMC) for Posteriors of Bounded Parameters"/>
<meta name="twitter:description" content="This is largely a note to my past-self on how to easily use Markov Chain Monte Carlo (MCMC) methods for Bayesian inference when the parameter you are interested in has bounded support. The most basic MCMC methods involve using additive noise to get new draws, which can cause problems if that kicks you out of the parameter space.
Suggestions abound to use the transformation trick on a bounded parameter \(\theta\), and then make draws of the transformed parameter."/>

  </head>

  <body class="ma0 avenir bg-near-white">

    
   
  

  <header>
    <div class="bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="/" class="f3 fw2 hover-white no-underline white-90 dib">
      Practically Insignificant
    </a>
    <div class="flex-l items-center">
      

      
      












    </div>
  </div>
</nav>

    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l flex-wrap justify-between mw8 center ph3">

    <header class="mt4 w-100">
      <p class="f6 b helvetica tracked">
          
        POSTS
      </p>
      <h1 class="f1 athelas mb1">Bounded Bayes: Markov Chain Monte Carlo (MCMC) for Posteriors of Bounded Parameters</h1>
      
      
      <time class="f6 mv4 dib tracked" datetime="2020-12-17T00:00:00Z">December 17, 2020</time>
      
      
    </header>

    <section class="nested-copy-line-height lh-copy serif f4 nested-links nested-img mid-gray pr4-l w-two-thirds-l">
<link href="/rmarkdown-libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="/rmarkdown-libs/anchor-sections/anchor-sections.js"></script>


<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script>
<!--

Time spent:

161220, 10 min
171220, 40 min

-->
<p>This is largely a note to my past-self on how to easily use Markov Chain Monte Carlo (MCMC) methods for Bayesian inference when the parameter you are interested in has bounded support. The most basic MCMC methods involve using additive noise to get new draws, which can cause problems if that kicks you out of the parameter space.</p>
<p>Suggestions abound to use the transformation trick on a bounded parameter <span class="math inline">\(\theta\)</span>, and then make draws of the transformed parameter. But I could not easily find a worked example. So here’s that worked example.</p>
<p>Suppose we want to simulate from the posterior distribution of a parameter <span class="math inline">\(\theta\)</span> with a bounded parameter space. Select an invertible function <span class="math inline">\(g\)</span> that maps the bounded parameter space to the real line, i.e.
<span class="math display">\[\psi = g(\theta) \text{ where } g : \Theta \to \mathbb{R}.\]</span>
Then simulate from the posterior density <span class="math inline">\(f_{\Psi \mid \mathbf{X}}(\psi \mid \mathbf{x})\)</span> in terms of the transformed parameter <span class="math inline">\(\psi\)</span>. By the transformation theorem,
<span class="math display">\[\psi \mid \mathbf{X} = \mathbf{x} \sim f_{\Psi \mid \mathbf{X}}(\psi \mid \mathbf{x}) = \frac{f_{\Theta \mid \mathbf{X}}(g^{-1}(\psi) \mid \mathbf{x})}{|g&#39;(g^{-1}(\psi))|},\]</span>
where the posterior <span class="math inline">\(f_{\Theta \mid \mathbf{X}}\)</span> is known, at least up to the likelihood and prior.</p>
<p>Let’s use this to simulate a success probability for a binomial proportion from the posterior density <span class="math inline">\(f_{P \mid X}(p \mid x)\)</span>. Here, because <span class="math inline">\(p \in (0, 1)\)</span>, it’s natural to use the logit function to map from <span class="math inline">\((0, 1)\)</span> to <span class="math inline">\((-\infty, \infty)\)</span>:
<span class="math display">\[
\psi = \operatorname{logit}(p) = \log \frac{p}{1-p}
\]</span>
The transformation theorem then gives the posterior in terms of <span class="math inline">\(\psi\)</span> as:
<span class="math display">\[
\begin{aligned} 
  f_{\Psi \mid X}(\psi \mid x) &amp;= \frac{f_{P \mid X}(\operatorname{logit}^{-1}(\psi) \mid x)}{|\operatorname{logit}&#39;(\operatorname{logit}^{-1}(\psi))|}  \\
  &amp;= f_{P \mid X}(p \mid x) \times p(1-p)
  \end{aligned}
\]</span>
where
<span class="math display">\[
p = \operatorname{logit}^{-1}(\psi) = \frac{e^{\psi}}{1 + e^{\psi}}.
\]</span>
Sampling <span class="math inline">\(\psi\)</span> via a random walk and using the Metropolis-Hastings update rule now gives a chain in terms of <span class="math inline">\(\psi_{1}, \psi_{2}, \ldots, \psi_{B}\)</span>, which can be transformed via <span class="math inline">\(\operatorname{logit}^{-1}\)</span> into a chain in terms of <span class="math inline">\(p_{1}, p_{2}, \ldots, p_{B}\)</span>.</p>
<p>Let’s use the Jeffreys prior for <span class="math inline">\(P\)</span>: <span class="math inline">\(P \sim \operatorname{Be}(1/2, 1/2)\)</span>. In this case, we can easily work out the posterior, but let’s use a <a href="http://www.stats.org.uk/priors/noninformative/YangBerger1998.pdf">lookup table</a> put together by Ruoyong Yang and Jim Berger. The posterior distribution of <span class="math inline">\(P\)</span> given we observe <span class="math inline">\(x\)</span> successes in <span class="math inline">\(n\)</span> trials is <span class="math inline">\(P \mid X = x \sim \operatorname{Be}(x + 1/2, n - x + 1/2)\)</span>.</p>
<p>We know the posterior in terms of <span class="math inline">\(\psi = \operatorname{logit}(p)\)</span> is
<span class="math display">\[f(\psi \mid x) \propto f(x \mid \psi) f(\psi).\]</span>
Moreover, the posterior in terms of <span class="math inline">\(p\)</span> is</p>
<p><span class="math display">\[
\begin{aligned}  
    f(p \mid x) &amp;\propto f(x \mid p) f(p) \\
    &amp;= \texttt{dbinom(x, n, p)*dbeta(p, 1/2, 1/2)} 
\end{aligned}
\]</span>
And the posterior in terms of <span class="math inline">\(\psi\)</span> is just the posterior in terms of <span class="math inline">\(p\)</span>, modified by the multiplicative factor that takes care of the transformation from <span class="math inline">\(p\)</span> to <span class="math inline">\(\psi\)</span>
<span class="math display">\[
f(\psi \mid x) \propto f(p \mid x) p (1-p) \bigg\rvert_{p = \operatorname{logit}^{-1}(\psi)}
\]</span>
This is the posterior distribution we will make draws from using Metropolis-Hastings, i.e. given that we have <span class="math inline">\(\psi_{1}, \psi_{2}, \ldots, \psi_{t-1}\)</span>, generate a new potential draw <span class="math inline">\(\psi_{t} = \psi_{t-1} + \epsilon_{t}\)</span> where <span class="math inline">\(\epsilon_{t} \sim N(0, \sigma^{2})\)</span>, and compute
<span class="math display">\[ 
A = \min \left\{1, \frac{f(\psi_{t} \mid x)}{f(\psi_{t-1} \mid x)} \right\}.
\]</span>
We then accept <span class="math inline">\(\psi_{t}\)</span> with probability <span class="math inline">\(A\)</span>, or otherwise repeat the previous draw, i.e. <span class="math inline">\(\psi_{t} = \psi_{t-1}\)</span>.</p>
<p>Let’s define some functions for the logit and inverse logit functions:</p>
<pre class="r"><code>logit &lt;- function(p) log(p/(1-p))
invlogit &lt;- function(psi) exp(psi)/(1 + exp(psi))</code></pre>
<p>And to compute the numerator of the posterior distribution of <span class="math inline">\(\Psi \mid X = x\)</span>:</p>
<pre class="r"><code>posterior.numer &lt;- function(psi){
  p = invlogit(psi)
  dbinom(x, n, p)*dbeta(p, 1/2, 1/2)*p*(1-p)
}</code></pre>
<p>Now we’re ready to simulate from the posterior distribution via Metropolis-Hastings:</p>
<pre class="r"><code>set.seed(1) # For reproducibility

n &lt;- 2
x &lt;- 1

B &lt;- 100000

psis &lt;- rep(0, length(B))

for (b in 2:B){
  candidate.psi &lt;- psis[b-1] + rnorm(1, 0, 1)
  
  A &lt;- min(c(1, posterior.numer(candidate.psi)/posterior.numer(psis[b-1])))
  
  if (runif(1) &lt;= A){ # Accept candidate.psi with probability A
    psis[b] &lt;- candidate.psi
  }else{ # Reject candidate.psi and repeat psis[b-1]
    psis[b] &lt;- psis[b-1]
  }
}</code></pre>
<p>We can view the Markov chains for the posterior distributions in terms of either the <span class="math inline">\(\psi\)</span>s or the <span class="math inline">\(p\)</span>s:</p>
<pre class="r"><code>par(mfrow = c(2, 1))
plot(psis, type = &#39;l&#39;, xlim = c(1, 1000))
plot(invlogit(psis), type = &#39;l&#39;, xlim = c(1, 1000))</code></pre>
<p><img src="/post/2020-12-17-bounded-bayes_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>And the posterior distributions in terms of either the <span class="math inline">\(\psi\)</span>s or the <span class="math inline">\(p\)</span>s:</p>
<pre class="r"><code>par(mfrow = c(1, 2))
hist(psis, breaks = &#39;FD&#39;, main = &#39;&#39;)
hist(invlogit(psis), breaks = &#39;FD&#39;, freq = FALSE, xlim = c(0, 1), main = &#39;&#39;)
curve(dbeta(p, x + 0.5, n - x + 0.5), xname = &#39;p&#39;, add = TRUE, col = &#39;blue&#39;)</code></pre>
<p><img src="/post/2020-12-17-bounded-bayes_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>Clearly, the simulated draws match the true posterior, as they must.</p>
<ul class="pa0">
  
   <li class="list">
     <a href="/tags/statistics" class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">statistics</a>
   </li>
  
   <li class="list">
     <a href="/tags/reminders" class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">reminders</a>
   </li>
  
   <li class="list">
     <a href="/tags/worked-examples" class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">worked examples</a>
   </li>
  
</ul>
<div class="mt6">
      
      
      </div>
    </section>

    <aside class="w-30-l mt6-l">




  <div class="bg-light-gray pa3 nested-list-reset nested-copy-line-height nested-links">
    <p class="f5 b mb3">Related</p>
    <ul class="pa0 list">
	   
	     <li  class="mb2">
          <a href="/post/confidence-covid-mask-protection/">How Confident Are We that Masks &#34;Work&#34;? Confidence Functions and the DANMASK-19 Study</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/post/the-land-of-lost-significance-exact-tests-with-discrete-test-statistics/">Quantum Statistics: Exact Tests with Discrete Test Statistics</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/post/salvaging-lost-significance-via-randomization-randomized-p-values-for-discrete-test-statistics/">Salvaging Lost Significance via Randomization: Randomized \(P\)-values for Discrete Test Statistics</a>
        </li>
	    
    </ul>
</div>

</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="/" >
    &copy; 2023 Practically Insignificant
  </a>
    <div>











</div>
  </div>
</footer>

    

  <script src="/dist/js/app.3fc0f988d21662902933.js"></script>


  </body>
</html>
