<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>statistics on Practically Insignificant</title>
    <link>/tags/statistics/</link>
    <description>Recent content in statistics on Practically Insignificant</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 13 Sep 2021 00:00:00 +0000</lastBuildDate><atom:link href="/tags/statistics/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Inference via Stan for the Mean and Variance of a Gaussian (&#34;Normal&#34;) Population with Weakly Informative and Fiducial Priors</title>
      <link>/post/weakly-informative-and-fiducial-priors-for-gaussian-mean-and-variance/</link>
      <pubDate>Mon, 13 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>/post/weakly-informative-and-fiducial-priors-for-gaussian-mean-and-variance/</guid>
      <description>Preamble  Attenion Conservation Notice: I implement the now-standard Bayesian procedure for estimating a Gaussian mean and variance with weakly informative priors using Stan and make some connections to confidence distributions and fiducial inference. But without any of the details for this to make sense for a newcomer. For the former material, you are better served by page 73 of A First Course in Bayesian Statistical Methods by Peter Hoff or page 67 of Bayesian Data Analysis.</description>
    </item>
    
    <item>
      <title>Bounded Bayes: Markov Chain Monte Carlo (MCMC) for Posteriors of Bounded Parameters</title>
      <link>/post/bounded-bayes/</link>
      <pubDate>Thu, 17 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/bounded-bayes/</guid>
      <description>This is largely a note to my past-self on how to easily use Markov Chain Monte Carlo (MCMC) methods for Bayesian inference when the parameter you are interested in has bounded support. The most basic MCMC methods involve using additive noise to get new draws, which can cause problems if that kicks you out of the parameter space.
Suggestions abound to use the transformation trick on a bounded parameter \(\theta\), and then make draws of the transformed parameter.</description>
    </item>
    
    <item>
      <title>How Confident Are We that Masks &#34;Work&#34;? Confidence Functions and the DANMASK-19 Study</title>
      <link>/post/confidence-covid-mask-protection/</link>
      <pubDate>Fri, 04 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/confidence-covid-mask-protection/</guid>
      <description>Attention Conservation Notice: I use a recent study about COVID-19 as an excuse to demo an R package I am developing. Some other reasons not to read this: (a) I am not an epidemiologist and (b) I do not plan to explain confidence functions enough for the uninitiated to make sense of this post.
 Andrew Gelman had an interesting post recently about the DANMASK-19 trial out of Denmark.</description>
    </item>
    
    <item>
      <title>Quantum Statistics: Exact Tests with Discrete Test Statistics</title>
      <link>/post/the-land-of-lost-significance-exact-tests-with-discrete-test-statistics/</link>
      <pubDate>Sun, 28 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/the-land-of-lost-significance-exact-tests-with-discrete-test-statistics/</guid>
      <description>For those who ended up here looking for information about quantum statistical mechanics or particle statistics: apologies! But sometimes I feel like physicists took all the exciting names, so I’m stealing the term quanta as it relates to an observable that can take on only discrete (not continuous values). That has interesting consequences for inferential procedures based on such discrete (“quantum”) statistics, as we will see in this post.</description>
    </item>
    
    <item>
      <title>Salvaging Lost Significance via Randomization: Randomized \(P\)-values for Discrete Test Statistics</title>
      <link>/post/salvaging-lost-significance-via-randomization-randomized-p-values-for-discrete-test-statistics/</link>
      <pubDate>Sun, 28 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/salvaging-lost-significance-via-randomization-randomized-p-values-for-discrete-test-statistics/</guid>
      <description>Last time, we saw that when performing a hypothesis test with a discrete test statistic, we will typically lose size unless we happen to be very lucky and have the significance level \(\alpha\) exactly match one of our possible \(P\)-values. In this post, I will introduce a randomized hypothesis test that will regain the size we lost. Unlike a lot of randomization in statistics, the randomization here comes at the end: we randomize the \(P\)-value in order to recover the size.</description>
    </item>
    
  </channel>
</rss>
